import torch
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer
import soundfile as sf
import numpy as np
import warnings
import time
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import os
import re
from functools import partial

# Suppress warnings
warnings.filterwarnings("ignore")

# ------------------ Configuration ------------------
class TTSConfig:
    MODEL_NAME = "ai4bharat/indic-parler-tts-pretrained"
    VOICE_DESCRIPTION = "A professional female speaker Anu, speaks with clear pronunciation and moderate pacing"
    OUTPUT_FILE_SERIAL = "kannada_tts_serial.wav"
    OUTPUT_FILE_PARALLEL = "kannada_tts_parallel.wav"
    SILENCE_PAUSE = 0.15  # seconds between sentences
    
    # Parallel processing settings
    MAX_WORKERS = min(4, mp.cpu_count())  # Adjust based on your CPU cores
    BATCH_SIZE = 4
    
    # Generation config
    GENERATION_CONFIG = {
        'do_sample': True,
        'temperature': 0.95,
        'top_k': 50,
        'top_p': 0.95,
        'max_new_tokens': 1024,
        'num_beams': 1,
        'use_cache': True
    }

# ------------------ Text Preprocessing ------------------
number_map = {
    "0": "‡≤∏‡≥ä‡≤®‡≥ç‡≤®‡≥Ü", "1": "‡≤í‡≤Ç‡≤¶‡≥Å", "2": "‡≤é‡≤∞‡≤°‡≥Å", "3": "‡≤Æ‡≥Ç‡≤∞‡≥Å", "4": "‡≤®‡≤æ‡≤≤‡≥ç‡≤ï‡≥Å",
    "5": "‡≤ê‡≤¶‡≥Å", "6": "‡≤Ü‡≤∞‡≥Å", "7": "‡≤è‡≤≥‡≥Å", "8": "‡≤é‡≤Ç‡≤ü‡≥Å", "9": "‡≤í‡≤Ç‡≤≠‡≤§‡≥ç‡≤§‡≥Å",
    "10": "‡≤π‡≤§‡≥ç‡≤§‡≥Å", "50": "‡≤ê‡≤µ‡≤§‡≥ç‡≤§‡≥Å", "100": "‡≤®‡≥Ç‡≤∞‡≥Å", "2000": "‡≤é‡≤∞‡≤°‡≥Å ‡≤∏‡≤æ‡≤µ‡≤ø‡≤∞",
    "2024": "‡≤é‡≤∞‡≤°‡≥Å ‡≤∏‡≤æ‡≤µ‡≤ø‡≤∞ ‡≤á‡≤™‡≥ç‡≤™‡≤§‡≥ç‡≤§‡≥ç‡≤®‡≤æ‡≤≤‡≥ç‡≤ï‡≥Å", "50000000": "‡≤ê‡≤¶‡≥Å ‡≤ï‡≥ã‡≤ü‡≤ø"
}

kannada_digit_map = {
    "‡≥¶": "‡≤∏‡≥ä‡≤®‡≥ç‡≤®‡≥Ü", "‡≥ß": "‡≤í‡≤Ç‡≤¶‡≥Å", "‡≥®": "‡≤é‡≤∞‡≤°‡≥Å", "‡≥©": "‡≤Æ‡≥Ç‡≤∞‡≥Å", 
    "‡≥™": "‡≤®‡≤æ‡≤≤‡≥ç‡≤ï‡≥Å", "‡≥´": "‡≤ê‡≤¶‡≥Å", "‡≥¨": "‡≤Ü‡≤∞‡≥Å", "‡≥≠": "‡≤è‡≤≥‡≥Å", "‡≥Æ": "‡≤é‡≤Ç‡≤ü‡≥Å", "‡≥Ø": "‡≤í‡≤Ç‡≤≠‡≤§‡≥ç‡≤§‡≥Å"
}

def replace_numbers_with_kannada_words(text):
    def replace_match(match):
        number = match.group(0)
        return number_map.get(number, number)
    
    text = re.sub(r'\d+', replace_match, text)
    for digit, word in kannada_digit_map.items():
        text = text.replace(digit, word)
    return text

# ------------------ Model Management ------------------
class TTSEngine:
    def __init__(self, device_id=None):
        self.device = torch.device(f"cuda:{device_id}" if device_id is not None and torch.cuda.is_available() else "cpu")
        self.torch_dtype = torch.float16 if self.device.type == "cuda" else torch.float32
        self.model = None
        self.tokenizer = None
        self.description_tokenizer = None
        self.description_inputs = None
        
    def load_model(self):
        """Load model and tokenizers"""
        try:
            print(f"‚è≥ Loading model for device: {self.device}")
            self.model = ParlerTTSForConditionalGeneration.from_pretrained(
                TTSConfig.MODEL_NAME,
                torch_dtype=self.torch_dtype
            ).to(self.device)
            self.model.eval()
            
            # Try to apply optimizations
            try:
                from optimum.bettertransformer import BetterTransformer
                self.model = BetterTransformer.transform(self.model)
                print(f"‚úÖ BetterTransformer enabled for {self.device}")
            except ImportError:
                pass
                
        except Exception as e:
            print(f"‚ùå Error loading model for {self.device}: {e}")
            return False
            
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(TTSConfig.MODEL_NAME)
            self.description_tokenizer = AutoTokenizer.from_pretrained(self.model.config.text_encoder._name_or_path)
            
            # Pre-tokenize description
            with torch.no_grad():
                self.description_inputs = self.description_tokenizer(
                    TTSConfig.VOICE_DESCRIPTION,
                    return_tensors="pt",
                    max_length=512,
                    truncation=True
                ).to(self.device)
                
        except Exception as e:
            print(f"‚ùå Error loading tokenizers for {self.device}: {e}")
            return False
            
        return True
    
    def process_sentence(self, sentence):
        """Process a single sentence (serial mode)"""
        try:
            with torch.no_grad():
                inputs = self.tokenizer(
                    sentence,
                    return_tensors="pt",
                    max_length=512,
                    truncation=True
                ).to(self.device)

                audio = self.model.generate(
                    input_ids=self.description_inputs.input_ids,
                    attention_mask=self.description_inputs.attention_mask,
                    prompt_input_ids=inputs.input_ids,
                    prompt_attention_mask=inputs.attention_mask,
                    **TTSConfig.GENERATION_CONFIG
                )
                return audio.cpu().numpy().squeeze().astype('float32')
        except Exception as e:
            print(f"‚ö† Error processing sentence on {self.device}: {e}")
            return None

# ------------------ Parallel Worker Function ------------------
def parallel_worker(sentence_batch, worker_id):
    """
    Worker function for parallel processing
    Each worker gets its own model instance
    """
    try:
        # Set CUDA device for this worker if available
        if torch.cuda.is_available():
            torch.cuda.set_device(worker_id % torch.cuda.device_count())
        
        # Create engine for this worker
        engine = TTSEngine(worker_id % torch.cuda.device_count() if torch.cuda.is_available() else None)
        if not engine.load_model():
            return []
        
        results = []
        for sentence in sentence_batch:
            audio = engine.process_sentence(sentence)
            if audio is not None:
                results.append(audio)
        
        # Clean up GPU memory
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        return results
        
    except Exception as e:
        print(f"‚ùå Worker {worker_id} failed: {e}")
        return []

# ------------------ Processing Modes ------------------
def serial_processing(sentences, config):
    """Sequential processing - baseline for comparison"""
    print("üöÄ Starting SERIAL processing...")
    start_time = time.time()
    
    engine = TTSEngine()
    if not engine.load_model():
        return None
    
    full_audio = np.array([])
    sampling_rate = engine.model.config.sampling_rate
    
    for i, sentence in enumerate(sentences):
        print(f"üìù Processing sentence {i+1}/{len(sentences)}...")
        audio_arr = engine.process_sentence(sentence)
        
        if audio_arr is not None:
            full_audio = np.concatenate((full_audio, audio_arr))
            # Add pause between sentences (except after last one)
            if i < len(sentences) - 1:
                full_audio = np.concatenate((
                    full_audio,
                    np.zeros(int(config.SILENCE_PAUSE * sampling_rate))
                ))
    
    serial_time = time.time() - start_time
    print(f"‚úÖ Serial processing completed in {serial_time:.2f} seconds")
    return full_audio, serial_time

def parallel_processing(sentences, config):
    """True parallel processing using multiprocessing"""
    print("üöÄ Starting PARALLEL processing...")
    start_time = time.time()
    
    # Split sentences into batches for workers
    sentence_batches = []
    batch_size = max(1, len(sentences) // config.MAX_WORKERS)
    
    for i in range(0, len(sentences), batch_size):
        batch = sentences[i:i + batch_size]
        sentence_batches.append(batch)
    
    print(f"üîß Using {len(sentence_batches)} workers for {len(sentences)} sentences")
    
    # Use ProcessPoolExecutor for true parallelism
    full_audio = np.array([])
    sampling_rate = None
    
    try:
        with ProcessPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
            # Map batches to workers
            worker_func = partial(parallel_worker, worker_id=os.getpid())
            results = list(executor.map(worker_func, 
                                      sentence_batches, 
                                      range(len(sentence_batches))))
            
            # Combine results
            for batch_result in results:
                for j, audio_arr in enumerate(batch_result):
                    if audio_arr is not None:
                        full_audio = np.concatenate((full_audio, audio_arr))
                        # Add pause between sentences
                        full_audio = np.concatenate((
                            full_audio,
                            np.zeros(int(config.SILENCE_PAUSE * sampling_rate if sampling_rate else 24000))
                        ))
            
            # Get sampling rate from first successful result
            if results and results[0]:
                sampling_rate = 24000  # Default sampling rate
    
    except Exception as e:
        print(f"‚ùå Parallel processing failed: {e}")
        # Fallback to threading-based parallelism
        print("üîÑ Falling back to threading-based parallelism...")
        return threading_based_parallel(sentences, config)
    
    parallel_time = time.time() - start_time
    print(f"‚úÖ Parallel processing completed in {parallel_time:.2f} seconds")
    return full_audio, parallel_time

def threading_based_parallel(sentences, config):
    """Threading-based parallelism (for comparison)"""
    print("üîß Using THREADING-based parallelism...")
    start_time = time.time()
    
    engine = TTSEngine()
    if not engine.load_model():
        return None
    
    full_audio = np.array([])
    sampling_rate = engine.model.config.sampling_rate
    
    # Process in batches using threads
    for i in range(0, len(sentences), config.BATCH_SIZE):
        batch = sentences[i:i + config.BATCH_SIZE]
        
        with ThreadPoolExecutor(max_workers=min(len(batch), config.BATCH_SIZE)) as executor:
            batch_results = list(executor.map(engine.process_sentence, batch))
        
        for j, audio_arr in enumerate(batch_results):
            if audio_arr is not None:
                full_audio = np.concatenate((full_audio, audio_arr))
                if j < len(batch_results) - 1 or i + config.BATCH_SIZE < len(sentences):
                    full_audio = np.concatenate((
                        full_audio,
                        np.zeros(int(config.SILENCE_PAUSE * sampling_rate))
                    ))
        
        print(f"‚úÖ Processed batch {i//config.BATCH_SIZE + 1}/{(len(sentences)//config.BATCH_SIZE) + 1}")
    
    threading_time = time.time() - start_time
    print(f"‚úÖ Threading-based processing completed in {threading_time:.2f} seconds")
    return full_audio, threading_time

# ------------------ Main Execution ------------------
def main():
    print("=" * 60)
    print("üéØ MULTICORE TTS PARALLEL PROCESSING DEMONSTRATION")
    print("=" * 60)
    
    # System info
    print(f"üíª System Information:")
    print(f"   CPU Cores: {mp.cpu_count()}")
    print(f"   GPU Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU Count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
    
    # Load and preprocess text
    kannada_text = """
    ‡≤ï‡≤®‡≥ç‡≤®‡≤° ‡≤≠‡≤æ‡≤∑‡≥Ü ‡≤≠‡≤æ‡≤∞‡≤§‡≤¶ ‡≤ï‡≤∞‡≥ç‡≤®‡≤æ‡≤ü‡≤ï ‡≤∞‡≤æ‡≤ú‡≥ç‡≤Ø‡≤¶ ‡≤Ö‡≤ß‡≤ø‡≤ï‡≥É‡≤§ ‡≤≠‡≤æ‡≤∑‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü.
    ‡≤á‡≤¶‡≥Å ‡≤¶‡≥ç‡≤∞‡≤æ‡≤µ‡≤ø‡≤° ‡≤≠‡≤æ‡≤∑‡≤æ ‡≤ï‡≥Å‡≤ü‡≥Å‡≤Ç‡≤¨‡≤ï‡≥ç‡≤ï‡≥Ü ‡≤∏‡≥á‡≤∞‡≤ø‡≤¶‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤∏‡≥Å‡≤Æ‡≤æ‡≤∞‡≥Å ‡≥´ ‡≤ï‡≥ã‡≤ü‡≤ø ‡≤ú‡≤®‡≤∞‡≥Å ‡≤Æ‡≤æ‡≤§‡≤®‡≤æ‡≤°‡≥Å‡≤µ ‡≤™‡≥ç‡≤∞‡≤Æ‡≥Å‡≤ñ ‡≤≠‡≤æ‡≤∑‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü.
    ‡≤ï‡≤®‡≥ç‡≤®‡≤°‡≤µ‡≥Å ‡≤Ö‡≤§‡≥ç‡≤Ø‡≤Ç‡≤§ ‡≤™‡≥ç‡≤∞‡≤æ‡≤ö‡≥Ä‡≤® ‡≤≠‡≤æ‡≤∑‡≥Ü‡≤ó‡≤≥‡≤≤‡≥ç‡≤≤‡≤ø‡≥ä‡≤Ç‡≤¶‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü, ‡≤á‡≤¶‡≤∞ ‡≤á‡≤§‡≤ø‡≤π‡≤æ‡≤∏ ‡≤∏‡≥Å‡≤Æ‡≤æ‡≤∞‡≥Å 2000 ‡≤µ‡≤∞‡≥ç‡≤∑‡≤ó‡≤≥‡≤∑‡≥ç‡≤ü‡≥Å ‡≤π‡≤ø‡≤Ç‡≤¶‡≤ï‡≥ç‡≤ï‡≥Ü ‡≤π‡≥ã‡≤ó‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü.
    ‡≤ï‡≤®‡≥ç‡≤®‡≤° ‡≤∏‡≤æ‡≤π‡≤ø‡≤§‡≥ç‡≤Ø‡≤µ‡≥Å ‡≤Ö‡≤∏‡≤Ç‡≤ñ‡≥ç‡≤Ø‡≤æ‡≤§ ‡≤ï‡≤µ‡≤ø‡≤ó‡≤≥‡≥Å ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤≤‡≥á‡≤ñ‡≤ï‡≤∞‡≤ø‡≤Ç‡≤¶ ‡≤∏‡≤Æ‡≥É‡≤¶‡≥ç‡≤ß‡≤µ‡≤æ‡≤ó‡≤ø ‡≤¨‡≥Ü‡≤≥‡≥Ü‡≤¶‡≥Å ‡≤¨‡≤Ç‡≤¶‡≤ø‡≤¶‡≥Ü.
    ‡≤ï‡≥Å‡≤µ‡≥Ü‡≤Ç‡≤™‡≥Å, ‡≤¨‡≥á‡≤Ç‡≤¶‡≥ç‡≤∞‡≥Ü, ‡≤ï‡≤æ‡≤∞‡≤Ç‡≤§‡≤∞‡≤Ç‡≤§‡≤π ‡≤Æ‡≤π‡≤æ‡≤®‡≥ç ‡≤∏‡≤æ‡≤π‡≤ø‡≤§‡≤ø‡≤ó‡≤≥‡≥Å ‡≤ï‡≤®‡≥ç‡≤®‡≤°‡≤ï‡≥ç‡≤ï‡≥Ü ‡≤Ö‡≤™‡≤æ‡≤∞ ‡≤ï‡≥ä‡≤°‡≥Å‡≤ó‡≥Ü ‡≤®‡≥Ä‡≤°‡≤ø‡≤¶‡≥ç‡≤¶‡≤æ‡≤∞‡≥Ü.
    """.strip()
    
    kannada_text = replace_numbers_with_kannada_words(kannada_text)
    sentences = [s.strip() + '.' for s in kannada_text.split('.') if s.strip()]
    
    print(f"üìä Dataset: {len(sentences)} sentences, {len(kannada_text)} characters")
    print(f"üîß Available workers: {TTSConfig.MAX_WORKERS}")
    
    # Run serial processing
    print("\n" + "="*50)
    serial_audio, serial_time = serial_processing(sentences, TTSConfig)
    if serial_audio is not None:
        sf.write(TTSConfig.OUTPUT_FILE_SERIAL, serial_audio, 24000)
        print(f"üíæ Serial output saved to: {TTSConfig.OUTPUT_FILE_SERIAL}")
    
    # Run parallel processing
    print("\n" + "="*50)
    parallel_audio, parallel_time = parallel_processing(sentences, TTSConfig)
    if parallel_audio is not None:
        sf.write(TTSConfig.OUTPUT_FILE_PARALLEL, parallel_audio, 24000)
        print(f"üíæ Parallel output saved to: {TTSConfig.OUTPUT_FILE_PARALLEL}")
    
    # Performance analysis
    print("\n" + "="*60)
    print("üìä PERFORMANCE ANALYSIS")
    print("="*60)
    
    if serial_time and parallel_time:
        speedup = serial_time / parallel_time
        efficiency = (speedup / TTSConfig.MAX_WORKERS) * 100
        
        print(f"‚è± Serial Time:    {serial_time:.2f} seconds")
        print(f"‚è± Parallel Time:  {parallel_time:.2f} seconds")
        print(f"üöÄ Speedup:        {speedup:.2f}x")
        print(f"üìà Efficiency:     {efficiency:.1f}%")
        print(f"üîß Workers Used:   {TTSConfig.MAX_WORKERS}")
        
        if speedup > 1:
            print("‚úÖ Parallel processing provided performance improvement!")
        else:
            print("‚ö†Ô∏è Parallel overhead exceeded benefits for this workload")
    
    # Technical insights
    print("\n" + "="*60)
    print("üî¨ TECHNICAL INSIGHTS")
    print("="*60)
    print("1. SERIAL PROCESSING:")
    print("   - Single process, single model instance")
    print("   - Simple but utilizes only one CPU core")
    print("   - No parallel overhead")
    
    print("\n2. PARALLEL PROCESSING:")
    print("   - Multiple processes, multiple model instances")
    print("   - Utilizes multiple CPU cores")
    print("   - Higher memory usage (model replication)")
    print("   - Communication overhead between processes")
    
    print("\n3. OPTIMIZATION OPPORTUNITIES:")
    print("   - Model quantization for faster loading")
    print("   - Pipeline parallelism (tokenize/generate overlap)")
    print("   - Dynamic batching based on sentence length")
    print("   - GPU memory pooling for multiple workers")

if __name__ == "__main__":
    # Required for Windows multiprocessing
    if os.name == 'nt':
        mp.freeze_support()
    
    main()